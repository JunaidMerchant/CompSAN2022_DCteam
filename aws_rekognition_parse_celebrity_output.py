# -*- coding: utf-8 -*-
"""ParseCelebrityRekognition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QGL2Oc85j93HzEVffQ9obJcH52-JIS3u
"""

# Import standard libraries
import pandas as pd, json, os, glob

# This will mount your local google drive & change into the right directory
# will need to authorize connection to drive
from google.colab import drive
drive.mount('/content/gdrive')
os.chdir("/content/gdrive/My Drive/CompSAN/RekognitionOutputs")

"""# This is for the celebrity face rekognition"""

# read in celebrity json (CJ) files
CJ1=json.load(open('CelebrityTimeSorted_pt1.json','r'))
CJ2=json.load(open('CelebrityTimeSorted_pt2.json','r'))
CJ3=json.load(open('CelebrityTimeSorted_pt3.json','r'))

# Then combine the lists of celebrity items (timestamps)
AllCelebs=CJ1['Celebrities']
AllCelebs.extend(CJ2['Celebrities'])
AllCelebs.extend(CJ3['Celebrities'])

# Create long dataframe with all celebrity data that includes: 
# time stamp (in seconds), actor, confidence, width (of bounding box in pixels), 
# height (bounding box), area (bounding box), 
# area_percent (area of bounding box as percent of frame area), brightness, sharpness
#
# Note that height/width values in json file are ratio of height/width of frame, 
# which are in pixels. So we first need to get the number of pixels of 
# the height, width, and area of the frame!
H=CJ1['VideoMetadata']['FrameHeight']
W=CJ1['VideoMetadata']['FrameWidth']
A=H*W

# create empty dataframe w NumRow, and proper column names
LongDf=pd.DataFrame(index=list(range(len(AllCelebs))),columns=['time_stamp','actor','confidence','width','height','area','area_percent','brightness','sharpness'])

# Loop through AllCelebs and populate the dataframe
for index, event in enumerate(AllCelebs):
  LongDf.at[index, 'time_stamp'] = event['Timestamp']*.001 # convert ms to s
  LongDf.at[index, 'actor'] = event['Celebrity']['Name']
  LongDf.at[index, 'confidence'] = event['Celebrity']['Confidence']
  LongDf.at[index, 'width'] = event['Celebrity']['Face']['BoundingBox']['Width']*W
  LongDf.at[index, 'height'] = event['Celebrity']['Face']['BoundingBox']['Height']*H
  LongDf.at[index, 'area'] = LongDf.at[index, 'width']*LongDf.at[index, 'height']
  LongDf.at[index, 'area_percent'] = (LongDf.at[index, 'area']/A)*100
  LongDf.at[index, 'brightness'] = event['Celebrity']['Face']['Quality']['Brightness']
  LongDf.at[index, 'sharpness'] = event['Celebrity']['Face']['Quality']['Sharpness']


# check new dataframe
LongDf=LongDf.astype({"confidence": float})
LongDf.to_csv('TimeByActorCharacteristics_long.csv',index=False)

# now we can reorganize these data into a dataframe that has unique time-stamp 
# rows, and columns for each character, and in each cell has the confidence rating
# to do so, we'll first need to extract all unique timestamps and characters
TimeStamps=list(LongDf.time_stamp.unique())

# for the actors, let's pick the top 10 most frequently occuring
# The counter library will make it easy - creates list with item and number of occurances of item as rows 
from collections import Counter
Actors10=[a[0] for a in Counter(LongDf.actor).most_common(10)]

# Create empty dataframe
WideDf=pd.DataFrame(index=TimeStamps,columns=Actors10).rename_axis('TimeStamp', axis=1)
WideDf=WideDf.astype(float)

# Loop through unique time-stamps
for index,event in enumerate(WideDf.index):

  # temporarily separate out the rows in LongDf corresponding w current timestamp
  CurTime=LongDf.loc[LongDf.time_stamp==event]

  # loop through each actors to search
  for a in range(len(Actors10)):
    # check to see if actor is on screen during current time stamp, and if so,
    # add confidence rating to the cell corresponding with that actor/time
    if sum(CurTime.actor==Actors10[a]) == 1:
      WideDf.iloc[index,a]=CurTime.loc[CurTime.actor==Actors10[a],'confidence'].astype(float)

    if sum(CurTime.actor==Actors10[a]) == 2:
      #WideDf.iloc[index,a]=pd.to_numeric(CurTime.loc[CurTime.actor==Actors10[a],'confidence'], errors='coerce')
      WideDf.iloc[index,a]=list(CurTime.loc[CurTime.actor==Actors10[a],'confidence'].astype(float))[0]

    if sum(CurTime.actor==Actors10[a]) > 2:
      print('------------------------------------')
      print('Index/TimeStamp has more than 2 matches:')
      print(index)
      print(event)


WideDf

WideDf.to_csv('TimeByTop10Actors.csv',index_label='TimeStamp')

"""# This is for the general face rekognition


"""

# change to where face detect files are
os.chdir("/content/gdrive/My Drive/CompSAN/RekognitionOutputs/FaceDetect")

# get all json files
Files=glob.glob("*.json")

# read in first json
J1=json.load(open(Files[0],'r'))

# add to all faces list
AllFaces=J1['Faces']

# loop through remaining files and add to allfaces list
for f in range(1,len(Files)):
  print(Files[f])
  JTEMP=json.load(open(Files[f],'r'))
  AllFaces.extend(JTEMP['Faces'])

# Note that height/width values in json file are ratio of height/width of frame, 
# which are in pixels. So we first need to get the number of pixels of 
# the height, width, and area of the frame!
H=J1['VideoMetadata']['FrameHeight']
W=J1['VideoMetadata']['FrameWidth']
A=H*W

# create empty dataframe w NumRow, and proper column names
LongFace=pd.DataFrame(index=list(range(len(AllFaces))),columns=['time_stamp','confidence','width','height','area','area_percent','brightness','sharpness'])

# Loop through AllCelebs and populate the dataframe
for index, event in enumerate(AllFaces):
  LongFace.at[index, 'time_stamp'] = event['Timestamp']*.001 # convert ms to s
  LongFace.at[index, 'confidence'] = event['Face']['Confidence']
  LongFace.at[index, 'width'] = event['Face']['BoundingBox']['Width']*W
  LongFace.at[index, 'height'] = event['Face']['BoundingBox']['Height']*H
  LongFace.at[index, 'area'] = LongFace.at[index, 'width']*LongFace.at[index, 'height']
  LongFace.at[index, 'area_percent'] = (LongFace.at[index, 'area']/A)*100
  LongFace.at[index, 'brightness'] = event['Face']['Quality']['Brightness']
  LongFace.at[index, 'sharpness'] = event['Face']['Quality']['Sharpness']


# check new dataframe
LongFace=LongFace.astype({"confidence": float})
LongFace.to_csv('AllFaces_long.csv',index=False)

# now we can reorganize these data into a dataframe that has unique time-stamp 
# rows, and columns for each character, and in each cell has the confidence rating
# to do so, we'll first need to extract all unique timestamps and characters
TimeStamps=list(LongFace.time_stamp.unique())


# Create empty dataframe. confidence, brightness, sharpness are all averaged in the case when there are multiple faces
# summed area is the sum of the area percent when there are multiple faces
WideFace=pd.DataFrame(index=TimeStamps,columns=['confidence','numfaces','summedarea','bright','sharp']).rename_axis('TimeStamp', axis=1)
WideFace=WideFace.astype(float)

# Loop through unique time-stamps
for index,event in enumerate(WideFace.index):

  # temporarily separate out the rows in LongDf corresponding w current timestamp
  CurTime=LongFace.loc[LongFace.time_stamp==event]

  # add number of faces
  WideFace.at[index,'numfaces']=len(CurTime)

  # add number of faces
  WideFace.at[index,'summedarea']=sum(CurTime.area_percent)

  # add number of confidence
  WideFace.at[index,'confidence']=CurTime['confidence'].mean()

  # add number of brightness
  WideFace.at[index,'bright']=CurTime['brightness'].mean()

  # add number of sharpness
  WideFace.at[index,'sharp']=CurTime['sharpness'].mean()

  
WideFace.to_csv('AllFaces_wide.csv',index_label='TimeStamp')
WideFace